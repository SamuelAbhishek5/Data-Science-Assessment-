{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cedcacd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a79d1670",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = \"data.xlsx - internship-data-1.csv\"  \n",
    "DATETIME_COL = \"time\"                 \n",
    "FREQ = \"5min\"                               \n",
    "VARIABLE_COLS = [\n",
    "    \"Cyclone_Inlet_Gas_Temp\",\n",
    "    \"Cyclone_Material_Temp\",\n",
    "    \"Cyclone_Outlet_Gas_draft\",\n",
    "    \"Cyclone_cone_draft\",\n",
    "    \"Cyclone_Gas_Outlet_Temp\",\n",
    "    \"Cyclone_Inlet_Draft\"\n",
    "]\n",
    "OUTPUT_DIR = \"outputs\"\n",
    "PLOTS_DIR = \"outputs/plots\"\n",
    "# Choose sensible thresholds (adjust with data inspection)\n",
    "TEMP_SHUTDOWN_TH = 40    # Example: below 40C means shutdown\n",
    "DRAFT_SHUTDOWN_TH = 20   # Example threshold for draft pressure\n",
    "# Remove short flickers (\"debounce\" for e.g. <30 min blips)\n",
    "MIN_SHUTDOWN_PERIOD = 6   # 6 x 5min = 30 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e895ab25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_timeseries(csv_path=CSV_PATH, dt_col=DATETIME_COL, freq=FREQ):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    if dt_col not in df.columns:\n",
    "        raise ValueError(f\"Datetime column '{dt_col}' not found. Columns: {df.columns.tolist()}\")\n",
    "\n",
    "    df[dt_col] = pd.to_datetime(df[dt_col], utc=True, errors=\"coerce\")\n",
    "    bad_dt = df[dt_col].isna().sum()\n",
    "    if bad_dt > 0:\n",
    "        print(f\"[WARN] {bad_dt} rows have unparseable timestamps and will be dropped.\")\n",
    "        df = df.dropna(subset=[dt_col])\n",
    "\n",
    "    df = df.sort_values(dt_col)\n",
    "    dup_count = df.duplicated(subset=[dt_col]).sum()\n",
    "    if dup_count > 0:\n",
    "        print(f\"[INFO] Found {dup_count} duplicate timestamps. Keeping first occurrence.\")\n",
    "        df = df[~df.duplicated(subset=[dt_col], keep=\"first\")]\n",
    "\n",
    "    df = df.set_index(dt_col)\n",
    "    start, end = df.index.min(), df.index.max()\n",
    "    full_index = pd.date_range(start=start, end=end, freq=freq, tz=\"UTC\")\n",
    "    df = df.reindex(full_index)\n",
    "\n",
    "    df.index.name = dt_col\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4499f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_gaps(df, freq=FREQ):\n",
    "    missing_rows = df.isna().all(axis=1).sum()\n",
    "    total_rows = len(df)\n",
    "    coverage = 100 * (1 - missing_rows / total_rows)\n",
    "\n",
    "    is_missing = df.isna().all(axis=1).astype(int)\n",
    "    seg_start = (is_missing.diff().fillna(0) == 1)\n",
    "    seg_end = (is_missing.diff().fillna(0) == -1)\n",
    "\n",
    "    starts = df.index[seg_start]\n",
    "    ends = df.index[seg_end]\n",
    "\n",
    "    if len(ends) and (len(starts) and ends[0] < starts[0]):\n",
    "        ends = ends[1:]\n",
    "    if len(starts) > len(ends):\n",
    "        ends = ends.append(pd.Index([df.index[-1]]))\n",
    "\n",
    "    gap_segments = []\n",
    "    for s, e in zip(starts, ends):\n",
    "        duration = (e - s)\n",
    "        gap_segments.append({\"start\": s, \"end\": e, \"duration\": duration})\n",
    "\n",
    "    gaps_df = pd.DataFrame(gap_segments)\n",
    "\n",
    "    print(f\"[GAPS] Grid rows: {total_rows}, fully-missing rows: {missing_rows}, coverage: {coverage:.2f}%\")\n",
    "    return gaps_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "923be21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def missingness_report(df, variable_cols=None):\n",
    "    cols = variable_cols or [c for c in df.columns]\n",
    "    rep = []\n",
    "    n = len(df)\n",
    "    for c in cols:\n",
    "        miss = df[c].isna().sum()\n",
    "        rep.append({\n",
    "            \"column\": c,\n",
    "            \"missing_count\": int(miss),\n",
    "            \"missing_pct\": 100 * miss / n if n else np.nan\n",
    "        })\n",
    "    return pd.DataFrame(rep).sort_values(\"missing_pct\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a4c1f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def duplicate_count(csv_path=CSV_PATH, dt_col=DATETIME_COL):\n",
    "    df = pd.read_csv(csv_path, usecols=[dt_col])\n",
    "    df[dt_col] = pd.to_datetime(df[dt_col], errors=\"coerce\", utc=True)\n",
    "    dups = df.duplicated(subset=[dt_col]).sum()\n",
    "    return int(dups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d52c5ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_ranges(df, variable_cols=None, iqr_k=1.5, mad_k=3.5):\n",
    "    cols = variable_cols or [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]\n",
    "    results = []\n",
    "    for c in cols:\n",
    "        series = df[c].dropna()\n",
    "        if series.empty:\n",
    "            results.append({\n",
    "                \"column\": c,\n",
    "                \"count\": 0,\n",
    "                \"iqr_low\": np.nan,\n",
    "                \"iqr_high\": np.nan,\n",
    "                \"mad_center\": np.nan,\n",
    "                \"mad_scale\": np.nan,\n",
    "                \"mad_low\": np.nan,\n",
    "                \"mad_high\": np.nan\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        q1, q3 = series.quantile([0.25, 0.75])\n",
    "        iqr = q3 - q1\n",
    "        iqr_low = q1 - iqr_k * iqr\n",
    "        iqr_high = q3 + iqr_k * iqr\n",
    "\n",
    "        median = series.median()\n",
    "        mad = (np.abs(series - median)).median()\n",
    "    \n",
    "        mad_std = 1.4826 * mad if mad > 0 else np.nan\n",
    "        mad_low = median - mad_k * mad_std if not np.isnan(mad_std) else np.nan\n",
    "        mad_high = median + mad_k * mad_std if not np.isnan(mad_std) else np.nan\n",
    "        results.append({\n",
    "            \"column\": c,\n",
    "            \"count\": int(series.shape[0]),\n",
    "            \"iqr_low\": float(iqr_low),\n",
    "            \"iqr_high\": float(iqr_high),\n",
    "            \"mad_center\": float(median),\n",
    "            \"mad_scale\": float(mad_std) if not np.isnan(mad_std) else np.nan,\n",
    "            \"mad_low\": float(mad_low) if not np.isnan(mad_low) else np.nan,\n",
    "            \"mad_high\": float(mad_high) if not np.isnan(mad_high) else np.nan,\n",
    "        })\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f11138b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_stats(df, variable_cols):\n",
    "    num_cols = [c for c in variable_cols if c in df.columns]\n",
    "    stats = df[num_cols].describe(percentiles=[0.05, 0.25, 0.5, 0.75, 0.95]).T\n",
    "    stats.to_csv(f\"{OUTPUT_DIR}/summary_statistics.csv\")\n",
    "    print(f\"[OK] Saved summary_statistics.csv with {len(num_cols)} variables.\")\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6629d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_matrix(df, variable_cols, method=\"pearson\"):\n",
    "    num_cols = [c for c in variable_cols if c in df.columns]\n",
    "    corr = df[num_cols].corr(method=method)\n",
    "    corr.to_csv(f\"{OUTPUT_DIR}/correlation_matrix.csv\")\n",
    "    print(f\"[OK] Saved correlation_matrix.csv using method='{method}'.\")\n",
    "    return corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "335a7d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_representative_week(df, variable_cols):\n",
    "    # Heuristic: choose first continuous 7-day window with minimal missingness\n",
    "    start = df.index.min()\n",
    "    end = start + pd.Timedelta(days=7)\n",
    "    week = df.loc[start:end, variable_cols]\n",
    "    # If week has many missing, slide forward in 7-day increments to find better coverage\n",
    "    best = week\n",
    "    best_missing = week.isna().mean().mean()\n",
    "    current_start = start\n",
    "    for _ in range(156):  # check up to ~156 weeks\n",
    "        s = current_start\n",
    "        e = s + pd.Timedelta(days=7)\n",
    "        if e > df.index.max():\n",
    "            break\n",
    "        candidate = df.loc[s:e, variable_cols]\n",
    "        miss = candidate.isna().mean().mean()\n",
    "        if miss < best_missing:\n",
    "            best_missing = miss\n",
    "            best = candidate\n",
    "        current_start += pd.Timedelta(days=7)\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c05034",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_representative_year(df, variable_cols):\n",
    "    # Use first calendar year present in index\n",
    "    df_local = df.copy()\n",
    "    # Handle timezone-aware vs naive\n",
    "    if df_local.index.tz is not None:\n",
    "        df_local = df_local.tz_convert(\"UTC\")\n",
    "    years = sorted({(ts.year) for ts in df_local.index})\n",
    "    if not years:\n",
    "        raise ValueError(\"No timestamps to select a year from.\")\n",
    "    y = years[0]\n",
    "    year_slice = df_local.loc[str(y), variable_cols]\n",
    "    return year_slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0bb21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_week_overlay(week_df):\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    # Normalize for overlay visualization (z-score)\n",
    "    norm = (week_df - week_df.mean()) / (week_df.std(ddof=0) + 1e-9)\n",
    "    for c in norm.columns:\n",
    "        plt.plot(norm.index, norm[c], label=c, linewidth=1.2)\n",
    "    plt.title(\"Representative Week (z-scored overlay)\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Z-score\")\n",
    "    plt.legend(loc=\"upper right\", ncol=2, fontsize=8)\n",
    "    plt.tight_layout()\n",
    "    out = f\"{PLOTS_DIR}/week_overlay.png\"\n",
    "    plt.savefig(out, dpi=200)\n",
    "    plt.close()\n",
    "    print(f\"[OK] Saved {out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ddd98672",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_year_subplots(year_df):\n",
    "    cols = [c for c in year_df.columns]\n",
    "    n = len(cols)\n",
    "    nrows = int(np.ceil(n / 2))\n",
    "    plt.figure(figsize=(16, 3.5 * nrows))\n",
    "    for i, c in enumerate(cols, 1):\n",
    "        ax = plt.subplot(nrows, 2, i)\n",
    "        ax.plot(year_df.index, year_df[c], linewidth=0.8)\n",
    "        ax.set_title(c)\n",
    "        ax.set_xlabel(\"Time\")\n",
    "        ax.set_ylabel(\"Value\")\n",
    "    plt.tight_layout()\n",
    "    out = f\"{PLOTS_DIR}/year_subplots.png\"\n",
    "    plt.savefig(out, dpi=200)\n",
    "    plt.close()\n",
    "    print(f\"[OK] Saved {out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "15f3054f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shutdown_analysis(df):\n",
    "    # Combine shutdown conditions across variables\n",
    "    shutdown = (\n",
    "        (df[\"Cyclone_Inlet_Gas_Temp\"] < TEMP_SHUTDOWN_TH) &\n",
    "        (df[\"Cyclone_Inlet_Draft\"] < DRAFT_SHUTDOWN_TH)\n",
    "    )\n",
    "\n",
    "    # Label consecutive shutdown periods\n",
    "    block = (~shutdown).cumsum() if shutdown.any() else pd.Series(0, index=df.index)\n",
    "    shutdown_periods = shutdown.groupby(block).transform(\n",
    "        lambda x: x.sum() if x.sum() >= MIN_SHUTDOWN_PERIOD else 0\n",
    "    )\n",
    "    shutdown = shutdown & (shutdown_periods > 0)\n",
    "\n",
    "    # Segment shutdown periods: Find start and end times\n",
    "    df[\"shutdown_flag\"] = shutdown.astype(int)\n",
    "    changes = df[\"shutdown_flag\"].diff().fillna(0)\n",
    "    starts = df.index[changes == 1]\n",
    "    ends = df.index[changes == -1]\n",
    "\n",
    "    if len(ends) and (len(starts) and ends[0] < starts[0]):\n",
    "        ends = ends[1:]\n",
    "    if len(starts) > len(ends):\n",
    "        ends = ends.append(pd.Index([df.index[-1]]))\n",
    "\n",
    "    shutdown_segments = []\n",
    "    for s, e in zip(starts, ends):\n",
    "        shutdown_segments.append({\n",
    "            \"start\": s,\n",
    "            \"end\": e,\n",
    "            \"duration_mins\": int((e - s).total_seconds() / 60)\n",
    "        })\n",
    "\n",
    "    shutdown_df = pd.DataFrame(shutdown_segments)\n",
    "    shutdown_df.to_csv(\"outputs/shutdownperiods.csv\", index=False)\n",
    "\n",
    "    # Total downtime and event count (across all years)\n",
    "    total_downtime_hrs = shutdown_df[\"duration_mins\"].sum() / 60\n",
    "    print(f\"Total downtime: {total_downtime_hrs:.1f} hours\")\n",
    "    print(f\"Number of shutdown events: {len(shutdown_df)}\")\n",
    "    return shutdown_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c0d6af70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_shutdowns(df, shutdown_df):\n",
    "    # Pick a year to visualize\n",
    "    year = df.index.year.min()   # first year available\n",
    "    year_df = df.loc[str(year)]\n",
    "\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.plot(year_df.index, year_df[\"Cyclone_Inlet_Gas_Temp\"], label=\"Cyclone_Inlet_Gas_Temp\", alpha=0.6)\n",
    "    # Add background color for shutdown periods\n",
    "    for _, row in shutdown_df[shutdown_df[\"start\"].dt.year == year].iterrows():\n",
    "        plt.axvspan(row[\"start\"], row[\"end\"], color=\"red\", alpha=0.2)\n",
    "    plt.title(f\"Cyclone_Inlet_Gas_Temp in Year {year} (Shutdowns Highlighted)\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Temperature (°C)\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"outputs/plots/full_year_shutdowns.png\", dpi=200)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f6afca85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_analysis(active_df, variable_cols, n_clusters=3):\n",
    "    feat_df = active_df[variable_cols]\n",
    "    feat_df['InletTemp_30min_mean'] = feat_df['Cyclone_Inlet_Gas_Temp'].rolling(6).mean()\n",
    "    feat_df['InletTemp_30min_std'] = feat_df['Cyclone_Inlet_Gas_Temp'].rolling(6).std()\n",
    "    feat_df = feat_df.dropna()  # after rolling\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(feat_df)\n",
    "\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    labels = kmeans.fit_predict(X)\n",
    "    feat_df['cluster'] = labels\n",
    "    summary = feat_df.groupby('cluster')[variable_cols].agg(['mean', 'std', 'min', 'max', 'median', lambda x: x.quantile(0.25), lambda x: x.quantile(0.75)])\n",
    "    summary.to_csv('outputs/clustersummary.csv')\n",
    "\n",
    "    # Frequency and Duration of each state\n",
    "    feat_df['period'] = (feat_df['cluster'] != feat_df['cluster'].shift()).cumsum()\n",
    "    freq_table = feat_df.groupby('cluster')['period'].nunique()\n",
    "    duration_table = feat_df.groupby('cluster').apply(lambda x: x.shape[0]*5/60)  # hours, as 5min per row\n",
    "    print(freq_table)\n",
    "    print(duration_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b96f1680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DUPLICATES] Raw duplicate timestamps: 0\n",
      "Total downtime: 4418.8 hours\n",
      "Number of shutdown events: 53\n",
      "cluster\n",
      "0    8373\n",
      "1      73\n",
      "2     334\n",
      "3    8567\n",
      "Name: period, dtype: int64\n",
      "cluster\n",
      "0    20252.833333\n",
      "1     1496.083333\n",
      "2      888.416667\n",
      "3     4265.750000\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ly/1r5ypd8x4y105xpshs9ymw7h0000gn/T/ipykernel_955/1074930233.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  duration_table = feat_df.groupby('cluster').apply(lambda x: x.shape[0]*5/60)  # hours, as 5min per row\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Report duplicates in raw file\n",
    "    print(f\"[DUPLICATES] Raw duplicate timestamps: {duplicate_count(CSV_PATH, DATETIME_COL)}\")\n",
    "\n",
    "    df = load_timeseries(CSV_PATH, DATETIME_COL, FREQ)\n",
    "    for col in VARIABLE_COLS:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "    # Gap summary and segments\n",
    "    #gaps_df = summarize_gaps(df, FREQ)\n",
    "    #gaps_df.to_csv(\"outputs/gap_segments.csv\", index=False)\n",
    "\n",
    "    # Missingness by column\n",
    "    #miss_df = missingness_report(df, VARIABLE_COLS)\n",
    "    #miss_df.to_csv(\"outputs/missingness_report.csv\", index=False)\n",
    "\n",
    "    # Outlier ranges using IQR and MAD\n",
    "    #out_df = outlier_ranges(df, VARIABLE_COLS)\n",
    "    #out_df.to_csv(\"outputs/outlier_ranges.csv\", index=False)\n",
    "\n",
    "    #print(\"[DONE] Wrote outputs to outputs/ folder.\")\n",
    "     # Compute and save stats and correlations\n",
    "    #stats = summary_stats(df, VARIABLE_COLS)\n",
    "    #corr = correlation_matrix(df, VARIABLE_COLS, method=\"pearson\")\n",
    "\n",
    "    # Select representative slices\n",
    "    #week_df = pick_representative_week(df, VARIABLE_COLS)\n",
    "    #year_df = pick_representative_year(df, VARIABLE_COLS)\n",
    "\n",
    "    # Visualizations\n",
    "    #plot_week_overlay(week_df)\n",
    "    #plot_year_subplots(year_df)\n",
    "\n",
    "    # Optional quick-look prints\n",
    "    #print(stats.head())\n",
    "    #print(corr.round(3))\n",
    "\n",
    "    shutdown_df = shutdown_analysis(df)\n",
    "    #plot_shutdowns(df, shutdown_df)\n",
    "    active_df = df[df[\"shutdown_flag\"] == 0].copy()\n",
    "    cluster_analysis(active_df, VARIABLE_COLS, n_clusters=4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
